# No Redis, No Problem
## 제한된 인프라에서 고성능·고정합성 시스템을 실현한 실험형 백엔드 구조 설계




> 이 프로젝트는 단순한 서비스 구현이 아닌, **제약된 조건에서 최적의 설계를 도출하는 실험형 포트폴리오**입니다.
> Redis, Kafka, 분산락 없이도 **실시간 API와 배치가 공존하는 고성능 시스템을 설계할 수 있는가**를 검증하고자 했습니다.




## 🎯 **한 줄 요약**
**"Redis 없이도 월 100만 건 배치를 1.6분에 처리하면서, 실시간 서비스 장애 0건을 달성한 실험형 백엔드 구조"**


## 📊 **핵심 성과 (Before → After)**
| 지표 | Before | After     | 개선율        |
|------|--------|-----------|------------|
| 배치 처리 시간 | 81분 | 1.6분      | **96% ⬇**  |
| API 응답 속도 | 22.4초 | 0.43초     | **96% ⬇**  |
| 서비스 에러율 | 50% | 0%        | **100% ⬇** |
| 인프라 비용 | Redis 필요 | **0원 추가** | **비용 절감**  |


> ### 핵심 성과
> #### 1. 배치 처리 성능 98% 개선 (81.48분 → 1.3분)
> #### 2. 실시간 API 평균 응답속도 최대 96% 단축 (22.4s → 0.43s), 오류율 50% → 0%
> #### 3. Redis 없이도 Multi-WAS 환경에서  테스트 시나리오에서 정합성 충돌 0건 실현








## 🛠 기술 스택 및 실행 환경




| 구분         | 스택 / 도구                               |
|--------------|---------------------------------------|
| Language     | Java 17                               |
| Framework    | Spring Boot 3, MyBatis, JPA           |
| DB           | MySQL 8 (InnoDB)                      |
| Infra        | Docker, Nginx (Reverse Proxy), JMeter |
| 테스트/모니터링 | Apache JMeter, Docker Logs, visualVM  |




---




## 📌 0. 프로젝트 목표 및 핵심 과제




#### 본 프로젝트는 다음과 같은 목표를 가지고 시작되었습니다.
* **대용량 데이터 배치 처리:** 사용자 3만 명, 주문 100만 건에 대한 등급 및 쿠폰 발급 배치를 효율적으로 처리
* **실시간 API와의 동시성 보장:** 배치 실행 중에도 등급/쿠폰과 관련된 API 요청이 안정적으로 처리되도록 보장
* **인프라 제약 극복:** Redis 등 외부 메모리 저장소 없이, 주어진 인프라(WAS, DB) 내에서 문제 해결
*  **프로젝트는 크게 ` 배치 성능 최적화` 와 ` 동시성 및 정합성 보장` 이라는 두 가지 핵심 과제를 중심으로 진행되었습니다.**








---




## 1. 프로젝트 개요 및 테스트 환경




### 데이터 규모 (Dummy Data)
- 사용자: **30,000명**
- 주문: **1,000,000건**
- 주문상품: **2,000,000건**
- 상품: **40종**
- 카테고리: **8종**
- 배지: **240,000개 (사용자 × 카테고리)**




### 배치 정책
- 실행 시점: **매월 1일 00:00**
- 처리 내용:
  - 카테고리별 **주문수/주문액 기준** 배지 활성화 및 비활성화
  - 배지 결과 기반 **사용자 등급 갱신 및 로그 기록**
   - 등급에 따른 **쿠폰 발급**




### 테스트 시나리오
- **배치 처리**: 단일 스레드
- **실시간 요청 부하**:
  - 상품 조회
  - 상품 구매
  - 등급/배지/쿠폰 갱신 요청
- **부하 설정**: **500 스레드 × 10회 반복**




### 테스트 환경
- **CPU**: AMD Ryzen 9 5900HX (8코어 / 16스레드)
- **RAM**: 16GB (Docker 컨테이너 내 가용 메모리 4GB)




---




## 📈 제 1부 : 배치 성능 최적화 – JPA vs MyBatis




> #### 안정적인 서비스를 위해서는 배치 자체가 빨라야 하기에 먼저 배치 성능을 향상시키기 위해 노력함.
>#### 단순히 "MyBatis가 더 빠르다"는 막연한 이야기가 아닌, 두 프레임워크를 모두 초기 구현하고 성능을 비교하여 직접 체감하고자 함.




---




###  1.1성과 요약












```
JPA 초기     MyBatis 초기       MyBatis 최적화         JPA 병렬 최종




81분   ──────▶  48분     ──────▶      2분   ──────▶      1.3분 
```




















| 프레임워크 | 단계 | 핵심 전략             | 실행 시간(ms) | 주요 개선점          |
|------------|------|-----------------------|---------------|----------------------|
| MyBatis    | 1    | JDBC 배치            | 693,167       | 네트워크 I/O 해소    |
| MyBatis    | 2    | 쿼리 병합(SET-based) | 122,655       | DB 반복 실행 제거    |
| JPA        | 1    | IDENTITY 제약 확인    | 3,432,123     | 문제점 확인         |
| JPA        | 2    | UPDATE 우회          | 1,067,613     | 제한적 배치 동작     |
| JPA        | 3    | UUID PK 전환         | 250,019       | INSERT 배치 정상화   |
| JPA        | 4    | 병렬 처리(6스레드)    | 85,153        | 처리량 극대화       |
| JPA        | 5    | 랜덤 UUID           | 79,142        | DB 락 경합 해소     |




---




## 1.2 MyBatis 최적화 (122,655ms, ~2.0분)




### 개선 단계




---
**0단계 : 최초(2,911,744ms)**




---
**1단계 : JDBC 배치 도입 (693,167ms, ~11.5분)**
- **병목:** 단건 실행으로 인한 네트워크 왕복·커밋 비용
- **해결:** `ExecutorType.BATCH` 적용 → 쿼리 누적 후 일괄 전송
- **결과:** **2,911,744ms → 693,167ms (~4.2배 개선)**
---




**2단계 :  쿼리 병합(SET-based, 122,655ms)**
- **병목:** I/O 감소 후에도 DB **파라미터 바인딩·반복 실행** 남음
- **해결:** `foreach + CASE WHEN`으로 UPDATE/INSERT를 **단일 SQL**로 통합
- **결과:** **693,167ms → 122,655ms**




---
> **분석**
> MyBatis는 SQL을 직접 제어해 **극단적 성능**을 확보할 수 있었다.
> 이 **122,655ms** 성능은 **JPA 최적화의 목표 기준**이 되었다.




---




## 1.3 JPA 최적화 (79,142ms, ~1.3분)




### 개선 단계




**0단계 : `IDENTITY` 제약 (JPA: 3,432,123ms, 57분)**




- **병목 원인**
   - JPA 기본값인`IDENTITY` 정책은, **DB에서 PK를 생성**
   - JDBC 배치 insert를 해도 한 건마다 즉시 flush/commit 발생 → 실제로는 개별 insert와 다름없음




---
**1단계 : UPDATE 전략으로 우회 (1,067,613ms, 17분)**
- **고민 :** `INSERT`할때마다 PK생성이 문제라면, 차라리 미리 PK를 생성 해두고, `UPDATE`를 하자
- **구현 :**  `유저3만명` x `카테고리8개` =   `배지24만개` 미리 PK를 생성 뒤, `UPDATE`로 활성화 OR 비활성화 관리
- **결과:** **57분 → 17분**
- **한계:**
- `배지 = 카테고리*유저`와 같이 미리 알 수 있을 때만 적용가능
- 미리 파악 불가한 `쿠폰 발급`은 불가
- 초기 DB에 값이 없고, LOG를 이용하지 않는 비지니스 전략에서만 가능
---
**2단계 : UUID PK 전환 + 영속성 최적화 (250,019ms, 4.1분)**
- **이유 :** `IDENTITY`전략일 때, 배치를 사용할 방법 없음
- **전략 결정**:
  - **시간순 UUIDv7**를 PK로 전환
      - 이유: 향후 클러스터링 인덱스(정렬 기반) 활용, 조회 성능 개선까지 고려
      - WAS가 PK 생성 책임을 지면서 insert batch 가능해짐
  - **구조적 개선**:
      - N+1 문제 제거,
      - 더티체킹 필터링,
      - flush/clear 타이밍 조절(GC/메모리 최적화),
      - saveAll 일괄 트랜잭션,
      - DTO 기반 조회로 메모리 사용량 절감-
- **결과:** **250,019ms (~4.1분)**
---
**3단계 : 병렬 처리(6스레드, 85,153ms, 1.4분)**
- 8코어 환경에 맞춘 병렬화 적용 → CPU/IO 처리량 극대화
---
**4단계 : UUID 고도화(랜덤 UUID, 79,142ms, 1.3분)**
- **문제:** 병렬 스레드 도입 후 DB에서 직렬화 확인
- **이유:** 시간 UUID  → 마지막 리프노드 핫스팟  → 갭락 경합(직렬화)
- **해결:** 랜덤 UUID → 인덱스 분산 → 병목 해소(+6초)
- **트레이드오프:**
- 리프 페이지 갱신 vs 마지막 리프노드 핫스팟
  - 6초 남짓 향상(리프 페이지 갱신 비용이 생각 이상 비쌈)
- 만약 실제 서비스였으면 조회 성능을 위해 시간순 UUID 채택
- 핫스팟 해소를 위해 UUIDv7에서 비트를 조금 수정해서 마지막 노드에서 분산




- **결과 :** 실험 목표가 **최단 배치 시간**이므로 랜덤 UUID 선택.




---




## 1.4 종합 분석 및 선택 이유




- **MyBatis:** SQL 직접 제어로 최고의 성능 달성. 대규모 배치 전용 환경에 적합.
- **JPA:** 초기엔 느렸으나 **PK 전략 전환→영속성 최적화→병렬화**로 **MyBatis보다 빠른 결과** 확보.




### **최종 선택: JPA**
- **이유:**
   - 초기 프로젝트 단계 → **유연성·타입 안정성·빠른 변경 대응** 중시
  - **총소유비용(TCO)** 관점에서 생산성과 유지보수성을 확보
  - 성능 최적화가 절대적 목표가 되면 MyBatis로 전환 가능성 열어둠




> **결론**
> 단순한 성능 비교가 아닌, **성능·생산성·유지보수성** 균형을 찾는 여정이었다.
> JPA 선택은 **지속 가능한 개발 환경 구축**을 위한 전략적 판단이었다.
---




## 제 2부: 실시간 API 동시성 문제 해결 (Single-WAS 환경)
> **문제:** 배치 성능 최적화 후, Jmeter 시나리오 부하 테스트에서 **커넥션 풀 고갈**과 **Socket Timeout** 에러가 발생
### 동시성 전략 요약표




| 전략명         | 제어 위치    | 장점        | 단점         | 선택 여부      |
|-------------| -------- | --------- | ---------- | ---------- |
| DB Timeout  | DB       | 빠른 에러 반환  | 근본 해결 X    | ❌          |
| DB 플래그 테이블  | DB       | Lock 없음   | I/O 병목     | ❌          |
| WAS 메모리 플래그 | WAS      | 빠름, 경합 최소 | 멀티 WAS 미지원 | ⭕ (단일 WAS) |
| 논리 분기       | WAS + DB | 확장성, 정합성  | 멱등성 처리 필요  | ⭕ (최종 선택)  |




### 섹션 2.1 문제 정의
**배치API가 락소유 - 실시간API락대기 - 스레드, 커넥션 풀 고갈 - DB Lock으로 인한 '장애 전파**




### 섹션 2.2. 해결 전략: 최적의 제어 지점(Control Point)
> **DB Lock이 문제의 근원이라는 점에서, 제어권을 어디에 둘 것인지에 대한 다각적인 고민을 시작했습니다.**




#### [대안 1] ~~DB 레벨 제어~~ (기각):




- **시도:** `innodb_lock_wait_timeout` 같은 DB 설정을 줄여 Lock 대기 시간을 단축하는 방법.




- **문제점:** 이는 실패를 더 빨리 반환할 뿐, 근본적인 Lock 경합 자체를 해결하지 못함. 오히려 사용자에게 더 잦은 에러를 보여주어 경험만 해치는 방식이라 판단.




#### [대안 2] ~~DB를 이용한 플래그 제어~~ (기각):




- **시도:** **Lock 대신**, 배치 진행 상태를 나타내는 **플래그 테이블을 DB**에 만드는 방법.




- **문제점:** 모든 실시간 API 요청마다 플래그 확인을 위해 **DB I/O**가 발생. 이는 트래픽이 많아질수록 DB가 새로운 병목 지점이 될 것이 명백.




#### [최종 선택] WAS 레벨 제어:




- **결론:** 가장 효율적인 제어 지점은 요청이 DB에 도달하기 전인 WAS라고 판단.




- **설계 :** 외부 인프라 없이도 락 경합 시 서비스 중단을 피하기 위해, WAS **로컬 메모리의 플래그**로 충돌 위험 요청만 **선별 지연**한다.
- **1 단계 :  전역 플래그** 배치 시작 시 전체 API 차단
  - **한계 :** 동시성 최악
- **2 단계 : 개별 플래그**
   - **배치 시작 :** **[전역 플래그 ON]** -> 모든 관련 API 차단 (짧은 시간)
  - **대상 집계 :** **[집계 쿼리 실행]** -> 배치로 수정될 유저/배지 ID 목록 확보
  - **플래그 전환**: **[개별 플래그 ON] & [전역 플래그 OFF]** -> 집계된 대상에 대해서만 API 차단, 나머지 유저는 즉시 서비스 이용 가능
  - **배치 종료:** **[개별 플래그 OFF]** -> 모든 차단 해제
     이 방식으로 정합성, 자원 보호, UX 사이의 균형을 맞출 수 있었습니다.




- **제약:**  외부 라우팅 인프라(예: 유저기반 스티키 라우팅) 없이 **특정 유저를 특정 WAS로 고정할 수 없다.** 따라서 충돌 예측 시 **HTTP 202 Accepted**와 `Retry-After`를 반환해 **즉시 처리 대신 재시도**를 유도한다. 이는 **백프레셔 전략**이지, 처리 보장을 의미하지 않는다.
---




#### WAS 메모리 기반 - 실제 설계·실험




```
매달 1일 00시 배치시작                    집계쿼리                        배치 로직 실행                배치 청크 종료




 [전역 플래그]     --->   [누구의 무엇을 수정해야하나]  --->  [개별 플러그 ON][전역 플래그 OFF]  ---> [해당 플래그 해체]




전체 사용자 API 차단                 전체 사용자 API 차단                 배치 대상 API 차단
```




- **실제 효과:**
   - JMeter 부하 테스트에서 커넥션 풀, 락 경합, 서비스 장애 현상이 사라지고
  - 정합성·성능·사용성 모두 충족




# 제 3부 : Redis 없는 멀티 환경에서 동시성·정합성 확보 (Multi-WAS 환경) — 실패한 기록
## ⚠️ 본 파트는 정합성 보장 실패로 최종 폐기됨




> **중요 요약**
> 이 장은 **실패 기록**이다.
> 동적 논리 파티셔닝과 인터럽트 플래그는 **정합성 보장에 실패**하여 **폐기**했다.
> 그러나,
> **분산 환경에서의 반응성 향상, 멱등 복원, 상태 비소유 설계 등은 이후 4부 설계의 토대가 되었다.**
> 최종 해결책은 **4부의 스냅샷 기반 T0 설계**다.
---




## 3.1 문제 정의: 분산 환경에서 상태 동기화




- **핵심 질문:**
 “여러 WAS 인스턴스가 동시에 실행될 때, **배치 실행 여부**와 **중단 지점**을 어떻게 일관되게 관리할 것인가?”




---




## 3.2 고민 과정과 대안 검토




### [대안 1] ~~WAS 간 직접 통신~~ (기각)
- **아이디어:** 인스턴스 간 HTTP 호출로 상태 동기화
- **이유:** 네트워크 오버헤드, 장애 전파, 구현 복잡성 → **실패 가능성↑**




### [대안 2] ~~DB 중앙 플래그~~ (기각)
- **아이디어:** DB에 `Batch_Flag` 테이블 두고 상태 관리
- **이유:** DB 단일 병목, 락 경합 → **확장성↓**




### [최종 선택] ~~동적 논리 파티셔닝 (Dynamic Logical Partitioning)~~-폐기
- **발상 전환:** 상태를 **공유하거나 동기화하지 않는다.**
- 각 WAS는 **스스로 책임질 데이터만 처리**한다.
- **규칙:** `userId % totalWases == myIndex`
- **장점:** 중앙 제어 없이도 확장 가능, WAS 증감에 유연




---




## 3.3 초기 설계: 라운드 단위 분산 처리 (실패 기록)
- **용어**
   - **라운드(Round):** 배치의 큰 단계. `배지 → 등급 갱신 → 로그 기록 → 쿠폰 발급`.
- **동작**
   1) 부팅 시 WAS 등록 및 **myIndex** 산정.
  2) **논리 분할 규칙**으로 소유 결정: `userId % totalWases == myIndex` → **무상태 분산**.
  3) 각 **라운드 종료** 시 DB에서 활성 인스턴스 수 재조회 후 다음 라운드 분배에 반영.
- **운영 컴포넌트**
   - **StepExecution 로그:** 라운드별 `badgeDone/levelDone/couponDone`, `interrupted`, `wasIndex`, 시간 기록.
     목적: 완료 추적, 재시작 판단 근거.
  - **하트비트:** 주기적 생존 신호로 좀비 인스턴스 제외.
- **장점:** 구현 단순, 무상태 분산으로 충돌 확률 감소.
- **한계:** 스케일 변화 반영이 **라운드 경계**에만 일어나 **반응성 저하**(예: 라운드 30분이면 반영 지연 최대 30분).




---




## 3.4 진화 1: 청크 단위 반응성 향상 (실패 기록)
- **용어**
   - **청크(Chunk):** 라운드 내부 병렬 스레드의 작업 단위(예: 500 사용자).
- **변경**
   - 스케일 변화를 **청크 종료 시점**마다 확인해 반응성 향상.
  - **ChunkExecution 로그** 도입: `executionId, stepType, userIdStart~End, completed, recordedAt, wasId, restored` 등 기록.
     목적: **멱등성**(중복 방지)
- **상태 전이**
   - 청크 상태는 **PENDING → DONE** 단일 전이. 역전이 금지.
  - 동일 키(예: `(executionId, stepType, userIdStart)`) 중복 기록 금지.
- **지표 예시**
   - 완료율(=DONE/전체), 중복 차단률, 청크 평균 처리 시간.
- **장점:** 라운드 대비 **반응 속도↑**.
- **단점:** **매 청크 DB 조회**로 새로운 병목 발생.




---




## 3.5 진화 2: 인터럽트 플래그 기반 프로토콜




- **아이디어**
   - 스케일아웃 발생 시 새 WAS가 내부 **스케일 이벤트**를 기존 WAS에 HTTP로 통지.
  - 기존 WAS는 로컬 메모리에 **인터럽트 플래그**를 세팅.
  - 청크 루프는 **DB 폴링 대신 메모리 플래그만** 확인 후 **청크 경계에서 안전 중단**(flush 후 정지).
- **동작 흐름:**
   1. 청크 처리 → 완료 시 로그 기록
  2. 플래그 감지 시 flush & 중단
  3. 새 인스턴스가 로그 기반으로 이어서 실행(멱등성 보장)
- **장점:**
   - DB 부하 감소
  - 빠른 오토스케일링 반응
- **기술적 포인트:**
   - `chunk_execution_log`로 청크 범위·완료 여부 기록 - 복원에 사용
  - `step_execution_log`로 스텝(라운드) 상태·인터럽트 기록 - 복원에 사용




---




## 3.6 가장 큰 깨달음: 폐기한 이유
### 정합성이 깨졌다.




- **초기 전제:**
 “갱신될 배지만 잠그면 정합성을 확보할 수 있다.”
- **당시 테스트:**
   - JMeter로 500 Threads × 10 Loop 부하
  - HTTP 200 응답률 100%, Lock Timeout 0건 → **성공으로 판단**




- **실제 문제:**
   - 배치와 무관한 일부 유저가 배치 실행 중 **배지를 활성화/비활성화**
   - 개별 플래그 필터링이 되지 않아, 이 변경이 등급 산정에 반영
  - 결과적으로 **T0 시점이 깨져** 배치마다 **쿠폰 발급 수가 달라짐**




- **왜 늦게 깨달았나:**
   - JMeter 부하 테스트에서는 **오류(HTTP 에러)가 전혀 발생하지 않아** 정합성 문제가 없는 줄 알았다.
  - 오토스케일링 중단·복원까지 구현하고 최적화까지 마친 후, 실제 데이터 검증 단계에서 **쿠폰 발급 수가 기대치와 다른 것**을 발견했다.
  - 그제서야 "테스트 성공"과 "정합성 보장"은 다르며, **내 전제 자체가 잘못되었음**을 인지했다.




- **결론:**
   - 일부 유저만 잠그는 접근으로는 **정합성 보장 불가**.
  - 복잡한 논리적 분할·인터럽트 프로토콜은 **잘못된 가정 위에서의 노력**이었다.




- **교훈:**
   - 표면적인 성공(JMeter 통과)에 안주하면 근본 문제를 놓칠 수 있다.
  - 문제 정의가 잘못되면, 아무리 정교한 구현도 실패한다.
  - 이후 **스냅샷 기반 설계(T0 보장)** 로 전환했다.








# 제 4부 : 최종 해결책 – T0 스냅샷 기반 정합성 확보 (Snapshot-Based Architecture)


> **핵심 메시지** 
> “잠그지 않고도 정합성을 지킬 수 있다.” 
> 3부에서 실패한 원인은 ‘동시에 수정되는 라이브 데이터’에 의존한 것이다. 
> 해결책은 **시간을 고정하는 것**이었다.


---


## 4.1 문제 재정의: **정합성의 본질은 시점 보장**


- **질문 전환:** 
 “어떻게 잠글 것인가?” → **“어떤 시점을 기준으로 계산할 것인가?”**
- **핵심 인사이트:** 
 동시성 문제의 본질은 **경합**이 아니라 **관점 불일치**. 
 배치는 과거(T0), 실시간 API는 현재를 보면 된다.


---


## 4.2 설계 개요: **T0 스냅샷 + 결과 테이블**




### [대안 1] ~~MVCC 장기 스냅샷~~ (기각)


- **방식 :** DB의 REPEATABLE READ 격리 수준을 활용하여 긴 트랜잭션을 유지하는 방법
- **장점 :** 구현이 비교적 단순하다.
- **단점 :** 트랜잭션이 길어질수록 언두/리두 로그가 쌓인다. 장애 발생시 T0를 복구하기 어렵다.




### [대안 2] ~~테이블 복사~~ (기각)
- **방식 :** INSERT ... SELECT 쿼리를 사용해 배치에 필요한 데이터를 별도의 테이블로 통째로 복사하는 방법
- **장점 :** 읽기(배치)와 쓰기(API)가 물리적으로 완벽히 분리
- **단점 :** 데이터가 많을 경우 부담, 무엇보다 복사 중에 에러 발생 시 T0시점 깨짐




### [최종 선택] Copy-on-Write 로그 + 결과 테이블
- **개요 :** 위 두 방법의 단점을 회피하면서 핵심 목표를 달성하는 실용적인 방법. 산출(읽기)과 적용(쓰기) 단계를 명확히 분리하여 락 점유와 트랜잭션 길이를 최소화






## 섹션4.3 설계 진화 과정: T0 스냅샷 기반 구조로의 전환


### 초기: **T0 고정 + 즉시 쓰기**
- **방식:** 배치 시작 시점(T0)만 고정하고, 계산 직후 `badges` 테이블에 UPDATE.
- **장점:** 구조가 단순하고 구현이 빠름.
- **문제점:**
   - 대량 읽기와 쓰기가 혼합 → **트랜잭션 길어짐**, InnoDB **gap/next-key 대기** 증가.
  - 중간 장애 발생 시 **진행 상태 추적 불가** → 전체 재실행 필요.
  - 실시간 API와 배치 간의 락 경합으로 **응답 지연** 및 **성능 저하** 발생.


---




### 발전 1: **T0 스냅샷 + Copy-on-Write(COW) 로그**
- **개선점:**
   - 배치 시작 이후 실시간 API가 배지를 수정하면 최초 변경 시 `badge_log`에 원본 상태를 기록.
  - 배치는 **라이브 데이터 + 로그**를 조합해 T0 시점 상태를 재구성.
- **장점:**
   - API write 차단 없이 **T0 정합성 보장**.
- **한계:**
   - 여전히 산출 직후 즉시 `badges` UPDATE 방식 → **긴 트랜잭션** 유지.
  - 장애 시 **부분 복구 불가능**, 처음부터 재실행해야 함.
---




### 발전 2: **산출과 적용 분리**
- **개선점:**
   - **산출 단계(READ-ONLY):**
       - T0 기준으로 배지·등급·쿠폰 계산.
      - 결과를 `badge_results`, `level_results` 테이블에 `PENDING`으로 저장.
  - **적용 단계(WRITE-ONLY, 짧은 Tx):**
       - 배치 전용 WAS가 단일 리더로 선점.
      - `PENDING` 데이터를 소량씩 조회 후 짧은 트랜잭션으로 UPDATE → `APPLIED`.
- **효과:**
   - **락 보유 시간 단축**, **데드락 제거**.
  - **진행 상태 추적 가능(`PENDING/FAILED`)** → 중단 지점부터 재개.
  - **실시간 API와의 충돌 해소**: 배치 계산이 라이브 변경과 독립적.
- **결론:** 
 **T0 스냅샷 + COW 로그 + 산출/적용 분리**로 최종 확정.
---


## 4.4 구현 디테일
### Copy-on-Write(COW) 로그
- 테이블: `badge_log`
- 규칙: `(user_id, badge_id, period_key)` **최초 변경 1회** 기록(변경 전 상태)
- 인덱스: `user_id`, `badge_id`, `created_at`


### 결과 테이블
- 테이블: `badge_results`, `level_results`
- 필드: `execution_id`, `status(PENDING|APPLIED|FAILED)`, 유니크(`execution_id, user_id, badge_id`)


### 산출 단계(READ-ONLY)
1) T0 확정 → 2) 라이브+로그로 **T0 복원** → 3) 배지→등급→쿠폰 산출 → 4) `PENDING` 저장
- 대량 읽기. 락 대기 없음. 실시간 API 영향 없음.


### 적용 단계(WRITE-ONLY, 짧은 Tx)
1) 배치 전용 WAS **단일 리더 선점**
2) `PENDING` 키셋 페이징 조회
3) 짧은 Tx로 업데이트 → `APPLIED`
4) 실패는 `FAILED`로 마킹(부분 재시도 가능)
- **락 보유시간 최소화**, 데드락 제거, **중단 지점 확인 가능**.


### 장애·복구
- **상태 관리 기반**: `APPLIED`는 유지, `PENDING/FAILED`만 재적용 가능.
- **자동 재개 트리거는 미구현**(필요 시 `executionId`로 부분 재적용 가능).


### 정리
- **산출과 적용 분리**로 긴 트랜잭션과 락 경합 제거.
- **Copy-on-Write 로그**로 T0 기준을 유지하면서 실시간 API와 충돌 방지.
- **결과 테이블**로 진행 상태 추적과 멱등 복구 지원.
- 최종적으로 멀티 WAS 환경에서 안정적인 정합성을 확보하는 구조를 완성했다.


```
[실시간 API] → badges(Live)
                └─ badge_log(COW: 변경 전 상태)
[배치]
 [T0 복원] → results(PENDING)
         → [적용: 짧은 Tx] → badges(APPLIED)


```


# 제 5부 : 검증과 마무리 – 안정성 확인


> **목표:** 
> 최종 구조가 실제 환경에서도 안정적으로 동작함을 간단히 확인한다.


---


## 5.1 검증 방법


- **기본 기능 검증:** 배치 실행 후 결과 테이블 상태(`PENDING → APPLIED`)와 최종 데이터 일치 여부 확인.
- **부하 상황 검증:** JMeter **500명 동시 접근 × 10 loop**, 실시간 API 요청(조회/구매/배지·등급 변경/쿠폰 발급)과 배치 병행.
- **장애 상황 검증:** 배치 중 WAS 강제 종료 후 재기동, 상태 기반으로 복구 가능한지 확인.


---


## 5.2 주요 지표


| 항목                  | 결과        | 의미                         |
|---------------------|-----------|----------------------------|
| 배치 총 실행 시간        | 약 1~2분    | T0 기준 대량 처리 목표 충족     |
| 실시간 API 에러율       | 0%        | 부하 중에도 서비스 영향 없음     |
| Deadlock           | 0건       | 교착 상태 없이 처리 성공      |
| Lock Wait          | ≤1건     | 짧은 트랜잭션 구조로 대기 최소화 |


---


## 5.3 결론


- **정합성·동시성 보장:** 실시간 트래픽과 배치가 동시에 실행되어도 안정적.
- **복구 용이성:** 상태(PENDING/FAILED) 관리로 필요 시 중단 지점부터 재시작 가능.
- **최종 평가:** 
 제약된 인프라에서도 **DB 논리 설계만으로 고성능·고정합성 시스템을 달성**했다. 
 이는 초기 목표(성능, 동시성, 정합성 확보)를 모두 만족시킨 최종 아키텍처다










